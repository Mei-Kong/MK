 
                                                                               >>>>==妹控世界第一可爱==<<<<
                                                                                     
                                                                                     == 基本库 ==

=====================================================================================   URLLIB   ==============================================================================================

====Urllib 是python内置的HTTP请求库，有四个模块：
            request: 模拟HTTP请求
            error: 异常处理模块
            parse: URL处理方法
            rebotparser: 识别网站的robots.txt文件，判断哪些网站可爬，哪些不能
      
      
====发送请求：      
    1.urloprn()
        urllib 的 request 模块可以实现发送请求并得到相应
            urllib.request.urlopen('http...') 可以抓取网页的HTML源代码
            print(type(~~~))  可以输出相应网页的类型
            print(~~~.status())  输出响应的状态码
                     .getheaders()  输出响应的头信息
                     .getheader(‘’)  输出响应头的某个的值
                     .fileno  返回文件描述
                     .fileno()  返回文件描述符
 
        urllib() 函数的API封装：
            urllib.request.urlopen( url,  网址URL
    
                                    date=None,  以POST方式模拟表单提交方式
                                                如何需要添加该参数，需要使用 bytes() 方法将参数转化成bytes(字节流)类型
                                                    bytes() 第一个参数需要是 str(字符串)类型 
                                                                如果是字典，则需要用 urllib.parse 模块里的 urlencode() 将字典参数转换成字符串： urllib.parse.urlencode({  })  //里面是字典
                                                                
                                                            第二个参数是指定编码格式：encoding=''
                                                    
                                                bytes(urllib.parse.urlencode({ : }), encoding='utf8')
                                                    
                                                如果使用了，则请求方式不再是GET，而是POST
                                                http://httpbin.org/post    //可以用来测试POST请求   
                                                                  /get     //也可以测试GET，依此类推
                                                                               
                                    [timeout,]*,  设置超时时间，单位s，默认为全局时间    // time=
                                                  超时异常为 urllib.error.URLError 属于urllib.error模块
                                                  可以用try except语句跳过超时页面，使用 isinstance() 对比类型用socket模块的socket.timeout
                                          
                                    cafile=None,  CA证书的文件   //请求HTTPS链接时会有用
                                    capath=None,  CA证书的路径   //请求HTTPS链接时会有用
                            
                                    cadefault=False,  已弃用，默认false
                                    context=None  必须是ssl.SSLContext类似，用来指定ssl设置
                                    )
                                
                                
    2.Request()
        如果请求需要加入多个信息，如Headers，则需要 Request() 来构建一个完整的请求，方法：
            class urllib.request.Request( url,  必选参数
                                        date=None,   如果要传，必须是bytes类型，如果是字典，先用 urllib.parse.urlencode() 编码
                                        headers={},  这是一个字典，请求头，请求时可以通过headers参数直接构造，也可以通过请求实例 add_header() 方法添加
                                                     添加请求头时最常用的方法是通过修改User_Agent来伪装浏览器，默认为Python-urllib
                                        origin_req_host=None,  指请求方的 host名称 或者 ip地址
                                        unverifiable=False,  表示这个请求是否是无法验证的，默认为False，意思是用户没有足够的权限来选择接收这个请求的结果，例如请求一些图片等，就为true，因为大部分不需要验证
                                        method=None  指示请求使用的方法，是一个字符串，比如 GET, POST, PUT 等 
                                        )
                                     
                                        例如：A = urllib.request.Request(url, data=data, headers=headers, method=method)    #这里变量 A 等于一个完整的URL请求,是一个构架好的URL
                                     
                                     
    3.高级用法：例如cookies处理，代理设置等
        使用 Handler 即可简单完成，理解为解决各种处理的集合，对应cookies的处理类，对应代理的处理类等
    
        在urllib.erquest模块里的 BaseHandler 是所有Handler的父类：
            它的子类有很多  例如  HTTPDefaultErrorHandler :用于处理HTTP响应错误，错误会抛向HTTPError类型错误
                               HTTPRedirectHandler  :用于处理重定向
                               HTTPCookieProcessor  :用于处理Cookies
                               ProxyHandler  :用于处理代理，默认为空
                               HTTPPasswordMgr  :用于管理密码，它维护了用户名和密码的表
                               HTTPBasicAuthHandler  :用于管理认证
                               等等 
                
        还有一个重要的类 OpenerDirector 称为 opener ，与urlopen类似，实际上urlopen就是uriilb提供的opener，urlopen处理的比较简单而已，opener实际上可以处理高级的功能 
        简单地说，需要用 Handler 创建个性化的 opener 实现高级功能，功能都集成在 Handler 。
        build_opener 可理解为实现高级操作 Handler
    
    
        *验证：需要输入账号密码才能访问的页面
                引用模块类例如    from urllib.request import HTTPBasicAuthHandler（用于管理认证）, 
                                                           HTTPPasswordMgrWithDefaultRealm（可理解为一张表）, 
                                                           build_opener（用于发送验证，仅仅是发送）
                         
                例如：p = HTTPPasswordMgrWithDefaultRealm()    #使P成为一张用于验证的表
                     p.add_password(None, url, username, password)    #给P添加参数  .add_password(变量, 变量, ...)
                     handler = HTTPBasicAuthHandler(p)    #用HTTPBasicAuthHandler()使P形成验证操作，
                     opener = build_opener(handler)    #这一步build_opener是发送请求，只是验证而已，相当于实现这个功能
                     result = opener.open(url)   #当验证成功后，用.open可以打开网页获取代码，这里相当于urlopen
                     print(result.read().decode('GBK'))    #打印代码
                                     
        *代理 
            引用模块不多说
            ProxyHandler()创建一个对象，它是一个字典
                    a = ProxyHandler({                            #添加代理IP，字典类型
                            ‘http’: ‘http://192.168.1.1:1111’
                            ‘https’: ‘http://192.168.1.1:1112’
                           ......
                    })
                    opener = bulid_opener(a)     #这里直接openenr，之后.open() 
                
    
        *Cookies:读写cookies
                用cookie进行模拟登陆和访问，需要引用用 http.cookiesjar 中的 Cookiejar() 库
                再使用 urrlib.request 的 HTTPCookieProcessor() 处理Cookies
                写好获取命令后，就是基本的构建操作了，handler ———— opener ———— .open()
                注意：CookiesJar() 的对象是个字典，输出时常常只需要 name 和 value

                    cookie = http.cookiejar.CookieJar()    #创建CookJar() 的对象，是个字典
                    handler = urllib.request.HTTPCookieProcessor(cookie)    #指定操作
                    opener = urllib.request.build_opener(handler)    #构建opener
                    response = opener.open('http://www.baidu.com')    #打开url，实现opener操作
                    for a in cookie:    #此时输出response是源代码，获取cookies的话应该是遍历输出 cookie 字典变量，可只选name 和 value
                        print(a)                         
                                     
                cookie实际上是以文本形式保存的，那么输出时可以输出成文件格式，用 .save 保存  .load 读取
            
                    filename = 'cookies.txt'    #创建txt
                    cookie = http.cookiejar.MozillaCookieJar(filename)    #参数为文件名时自动创建文件
                    handler = urllib.request.HTTPCookieProcessor(cookie)
                    opener = urllib.request.build_opener(handler) 
                    response = opener.open('http://www.baidu.com') 
                    cookie.save(ignore_discard=True, ignore_expires=True)    #save将cookies保存到txt
                
                    ignore_discard 的意思是即使cookies将被丢弃也将它保存下来
                    ignore_expires 的意思是如果cookies已经过期也将它保存并且文件已存在时将覆盖
            
            
                CookieJar有两个子类：
                            MozillaCookieJar ：保存成Mozilla型浏览器的Cookie格式    http.cookiejar.MozillaCookieJar()
                            LWPCookieJar ：保存成LWP型浏览器的Cookie格式    http.cookiejar.MozillaCookieJar.LWPCookieJar()
                                     
                当需要调用本地Cookies文件时，用 load() 读取     
                            cookie.load(‘filename’, ignore_discard=True, ignore_expires=True)
                            这一步需要放在创建 handler 之上
                                     
                                     
====处理异常：
    1.URLError  
        它是 urllib 库的 eroor 模块，继承OSError，是error的基类
        他有一个属性 reason 返回错误的原因
               print(a.reason)
           
           
    2.HTTPError
          它是 URLError 的子类，专门处理 HTTP请求错误
          有三个属性： code： 返回HTTP的状态码
                    reason： 与父类一样，返回错误原因
                    headers： 返回请求头
                 
                print(a.reason, a.code, a.headers)
             
          如果想知道错误的类型，可以用上面的方法： print(type(a.reason))
       
       
====解析链接
    1.urlparse()
        实现 URL 的识别和分段，返回结果为ParseResult类型的对象，是一个元组，它分为6个部分
        scheme：协议    netloc：域名      path：访问路径
        params：参数    query：查询条件   fragment：锚点
      
            https://www.baidu.com/index.html;useraa?id=10#comment
            scheme//    netloc   /   path   ;params?query#fragment
                                     
        urlparse() 的API：
        urllib.parse.urlparse( urlstring: 必选，待解析的URL
                               scheme: 默认的协议，当URl没有带协议信息时才生效，将这个参数作为默认协议，
                               allow_fragments: 意为‘不忽略’fragment，设置为false时忽略，被忽略时会被解析为 path，params 或者 query 
                            )
      
      
    2.urlunparse()  
        对立方法，即组合构造URL
        它接受的参数是一个迭代对象，长度必须为6，否则报错
      
      
    3.urlsplit()
        与 urlparse() 类似，不过它把 params 合并到 path 中，返回5个结果
        类型同样是元组
                                     
                
    4.urlunsplit()
        对立方法，5个参数拼接
                                     
  
    5.urljoin()
        提供一个base_url（基础链接）作为第一个参数，将新链接作为第二个参数
        该方法只会解析 base_url的scheme,netloc,path 并将缺失的部分补充补充内容从新链接提取
        注意：只解析base_url的前三个，就是说base_url的后面部分将被忽略，从新链接中提取
  
  
    6.urlencode()
        将一个字典序列化成GET请求
                a={'name':'ABC', 'age':'10'}
                base_url='http://www.baidu.com?'
                url=base_url + urlencode(a)    #生成GET请求
            
            
    7.parse_qs()
        对立方法，将GET请求序列化转回字典
  
  
    8.parse_qsl()
        用于将url的参数部分转化为元组组成的列表
                query = 'name=AAA&age=10'
                print(parse_qsl(query))
            
                结果[('name','AAA'),('age','10')]    #结果是个列表，列表中每个元素都是一个元组（参数名，参数值）
            
            
    9.quote()
        可以将内容转化为URL编码的格式，特别是URL带中文的
        a = '妹控'
        url = 'http://www.baidu.com/s?wd=' + quote(a)
  
  
    10.unquote()
        对应方法，用来URL解码的
        url = ‘http://www.baidu.com/s?wd=%E5%A6%B9%E6%8E%A7’
        print(unquote(url))
  
  
====分析Robots协议
    1.Robots:
        Ribots协议也称作爬虫协议，机器人协议，全名：网络爬虫排除标准（Robots Exclusion Protocol）
        用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不能
        它通常是一个叫做 robots.txt 的文本文件，一般放在网站的根目录下
    
        爬虫首先检查网站根目录下是否存在 robots.txt 文件
        如果存在，则根据其定义的范围来爬取
        如果不存在，则访问可直接访问的页面
    
        robots.txt 文件主要内容：
            User-agent: *        #指定爬虫名，如果该爬虫没在这里，则不允许爬，* 指定所有
            Disallow: /         #指定不允许爬取的目录
            Allow/; /.../       #指定允许爬取的目录
        
    2.爬虫名称：
    
    
    3.robotparser:
        该模块提供一个类 RobotFileParser 
        它可以根据某网站的 robots.txt 文件来判断是否能爬
        只需要在构建方法里传入 robots.txt 的链接即可
        
            urllib.robotparser.RobotFileParser(url='')
            如果不传入默认为空，最后使用set_url()方法设置一下也可
        
        关于这个类的几个常用方法
            set_url()       :用来设置robots.txt文件的链接，如果在RobotFilePatrser()传入了，就不需要了
            read()          :读取并分析robots.txt 文件，如果不调用下面判断都会为false，所以一定要调用
            parse()         :解析robots.txt 文件，传入的参数是robots.txt 某行的内容，它会按照robots.txt。的语法规则来分析
            can_fetch()     :该方法传入入两个参数，第一个是URL-agent，第二个是要抓取的URL，返回内容是该搜索引擎是否能抓取这个URL，结果为true 和 false
            mtime()         :返回上次抓取和分析robots.txt的时间，对长时间分析和抓取的爬虫很有必要，用来定期检查和抓取最新额robots.txt
            modified()      :同样对长时间分析和抓取的爬虫很有必要，将当前时间设置为上次抓取和分析robots.txt 的时间
            
                例如：rp = RobotsFileParser('http://www.baidu.com/........')
                     rp.read()
                     print(rp.can_fetch('*', 'http://www.baidu.com/........'))
                     print(rp.can_fetch('*', 'http://www.baidu.com/......../........'))
    
    
==================================================================================== requests =================================================================================================

====简单来说，在urlib库中完成的get请求，post请求等等其他操作，都可以用requests库更简洁的完成
			例如： r = requests.get('https://www.baidu.com/')
				  print(type(r))5
				  print(r.text)
				  prinf(r.cookies)
				  
				  
	.text返回的是Unicode型的数据。
	.content返回的是bytes型也就是二进制的数据
				  
	
====基本用法
		1.GET请求
			很简单 :   r = requests.get('https://www.baidu.com/')
			      	  printf(r.text)
				  
			当需要附加额外的信息时，有两种方法：
			请求时直接添加内容：r = requests.get('https://www.baidu.com/get?....')
			
			但这样不人性，一般这些信息使用字典存储的，可直接用requests的 params 参数构造：
					data = {'name': 'AAA', 'age': '10'}
					r = requests.get('https://www.baidu.com/get?....', params=data)
				
			另外，网页返回类型实际上是str类型的，但它很特殊，是json格式的，如果想直接解析返回结果得到字典格式的话，可直接调用 json() 方法
				例如: print(r.json())
				需要注意的是，如果返回的不是json，则会出现解析错误，抛出 json.decoder.JSONDEcodeError 的异常
		
		
			*抓取页面：
				请求普通的网页，则获取相应的内容
					（空）
				
			*抓取二进制数据
				图片，视频，音频等媒体文件本质是由二进制码组成，只需要读取某个媒体文件，将其二进制码保存成 .xxx 文件
					例如： r = requests.get('https://github.com/favicon.ico')
					  	  with open('favicon.ico', 'wb') as f:
					  	  f.write(r.content)
					打开favicon.ico既看到图片
				
			*添加 Headers
				有些网页因为 robots 缘故，不添加某些信息是不能访问的，例如 Useragent，方法就是添加参数，不在赘述
					header = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}
					r = requests.get('https://www.zhihu.com/')
					 

		2.POST请求
			很简单，仅仅只是换个请求而已，只不过post需要list_num red">1.</div>   
                <div class="pic"><a href="http://prod提交数据，只需要把数据集合在字典里做参数就好，方法只是换成post
				data = {'name': 'AAA', 'age': '10'}
				r = requests.post('https://httpbin.org/post', data=data)    #get 换成 post
			
			
		3.响应
			例如：   r = requests.get('https://www.zhihu.com')
					print(type(r.status_code), r.status_code)
					print(type(r.url), r.url)
					
			其中，requests还提供了内置的状态码查询对象 requests.codes 
				例如：  r = requests.get('https://www.zhihu.com/')
				   	   exit() if not r.status_code == requests.codes.ok else print('OK')
				   
				       if not 如果不，状态码如果不相等200，则exit()，否则输出OK
				       相应的查询条件在128页
                       
                       5
====高级用法
        与urllib差不多，关键是简洁
        
        1.文件上传
            例如：    files = {'file': open('MEIKONG.jpeg', 'rb')}
                     r = requests.post('https://www.zhihu.com/', files=files)
                     
        2.Cookies
            获取cookies不再赘述
            
            可以直接用cookies来维持登录状态，这首先需要登录某个网站后，把cookie复制下来
            利用 headers参数 即可正常登录
            
            还可以通过cookies参数来设置，这需要构造RequestsCookieJar对象，这相对烦琐，书中132页
            
        3.会话维持
            每一次get() 或post() 请求都相当于用一个新的浏览器打开新的页面，这使得不能在同一会话中进行访问，也就是不能用同一个浏览器，打开新页面
            Session 可维持同一个会话
                例如：    requests.get('https://httpbin.org/cookies')             #请求一个网页的cookies
                         r = requests.get('https://httpbin.org/cookies/123')     #请求另一个网页的cookies
                         输出cookies，并不能获取当前的cookies
                         
                         r = requests.Session()                       #设置Session对象，注意大写
                         r.get('https://httpbin.org/cookies/123')     #请求第一个cookies
                         s = r.get('https://httpbin.org/cookies')     #第二个
                         输出cookies，成功返回当前cookies
        
        4.SSl证书验证
            requests 的 verify 参数用来控制是否检查证书，结果为 true 和 false
            有些没被官方CA机构信任的网站，访问时会出现 SSlError 证书验证错误，只需要将 verify 设置成false 即可跳过验证
                例如：
            
            不过，这样会发出一个建议指定证的警告，可以设置忽略警告来屏蔽，需要requests.packages 的 urllib3
                例如：    from requests.packages import urllib3
                         
                         urllib3.disable_warings()
                         response = requests.get('http://www.12306.cn', verify=False)
                         print(response.ststus_code)
                         
            还有其他屏蔽警告方式，书中135页
        
        5.代理设置
            相应的是 proxies参数 ，此对象是个字典，存放代理ip，格式：{'http': 'http://10.10.1.10:1111'}
            
            若代理需要 HTTP Basic Auth (HTTP基本认证) ，可使用类似http://user:password@host:port 这样的语法来设置代理
                例如：    proxies = {'http': 'http://user:password@host:port'}
            
            requests 还支持 SOCKS 协议代理，需要先安装socks库     pip3 install 'requests[socks]'
                例如：    proxies = {'http': 'socks5://user:password@host:port'}
                                             ↑这里换成了socks协议
            
            最后   requests.get('http://www.taobao.com', proxies=proxies)  发送请求
            
        6.超时设置
            相应参数为 timeout   (与urllib.request一样)
            
            详细书中136页
            
        7.身份验证
            对应参数 auth
                例如：  r = requests.get('http://localhost:5000', auth=('username', 'password'))
                
            requests 还提供了其他认证方式，例如 OAuth，需要安装对应的包  pip3 install requests_oauthlib
            元组组成账号密码，最后 get 发送，不赘述
            
        8.Prepared Request
            将请求表示为数据结构，在 requests 中叫做 Prepared Request
                例如：   url = 
                        data = 
                        headers = 
                        s = Session()                                                         #会话维持
                        request = Request('请求方式', 'url', 'data=data', 'headers=headers')   #Request 组合结构
                        prepped = s.prepare_request(request)                                #调用Session的prepare_request()方法将其转换成Prepared Request对象
                        r = s.send(prepped)                                                  #调用Session的 .send()方法发送
                        print(r.text)


==================================================================================== 正则表达式 =================================================================================================

====这个是处理字符串的强大工具，大概是，以某种表达方式去匹配字符串内容，返回符合正则表达式的字符串
    
    需要用到 re 库
    
        例如 格式：  [a-zA-Z]+://[^\s]
        
            \w        匹配字母，数字及下划线
            \W        匹配不是字母，数字及下划线的字符
            \s        匹配任意空白字符，等价与[\t\n\r\f]
            \S        匹配任意非空字符
            \d        匹配任意数字，等价于[0-9]
            \D        匹配任意非数字的字符
            \A        匹配字符串开头  
            \Z        匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串
            \z        匹配字符串结尾，如果存在换行，同时还会匹配换行符
            \G        匹配最后匹配完成的位置
            \n        匹配一个换行符
            \t        匹配一个制表符
            ^         匹配一行字符串的开头
            $         匹配一行字符串的结尾
            .         匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符
            [...]     用来表示一组字符，单独列出，比如[amk]匹配 a, m 或k
            [^...]    不在[]中的字符，比如[^abc]匹配除了a, b ,c 之外的字符
            *         匹配0个或多个表达式
            +         匹配1个或多个表达式
            ?         匹配0个或1个前面的正则表达式定义的片段，非贪婪方式
            {n}       精确匹配n个前面的表达式
            {n,m}     匹配n到m次由前面正则表达式定义的片段，贪婪方式
            a|b       匹配 a 或 b
            ( )       匹配括号内的表达式，也表示一个组
 
 
    1.match()
        match('正则表达式', '字符串')
        
        尝试从字符串的起始位置匹配正则表达式，就是从开头匹配
        如果匹配，就返回匹配成功的结果；如果不匹配，就返回None
        
            例如：    strs = 'hello 123 456 World_This is a Regex Demo'    #字符串
                     s = re.match('^hello\s\w{3}\s\w{3}\s', strs)         #返回匹配结果
                     print(s)
                     
                     打印输出结果，结果是 SRE_Match 对象，该对象有两个方法：
                     s.group()    输出匹配到的内容
                     s.span()     输出匹配的范围
                     
        *匹配目标
            如果想提取某段字符串的一部分，可以标记子表达式，每一个子表达式按顺序对应每一个分组，在.group()参数传入分组序号指明
                例如：  s = re.match('^hello\s(\w{3}\s\w{3})\s', strs)    #括号(\w{3}\s\w{3})表示数字那部分
                       print(s.group())                                  #返回整个正则表达式匹配的结果           
                       print(s.group(1))                                 #返回元组1，是括号部分           
            
        *通用匹配
            有时写\s\d...比较烦琐，有个万能匹配符可以用：.*，这样一段语句只需要提供开头和结尾就行了
            .  表示匹配任何字符，除换行符
            *  表示匹配前面的字符无限次
            
                例如：  s = re.match('^hello.*World', strs)    # .* 表示了hello到World之间的内容
        
        *贪婪与非贪婪
            看一个表达式： he.*(\d+).*Wor  ,group()预期结果的结果应该是数字部分123 456
            结果非预期，输出6
            
            这里就涉及到贪婪与非贪婪匹配的问题了
                在贪婪匹配下 .* 尽可能匹配更多字符，因为\d+至少是一个数字，那么将会到不是数字的前一步，也就是6，因为贪婪匹配尽可能匹配更多字符
                
                非贪婪匹配为 .*? 尽可能匹配较少的字符
                    例如：  he.*?(\d+).*Wor
                    
            需要注意，如果匹配的结果在字符串结尾， .*? 就有可能匹配不到任何内容了，因为结尾后面没有匹配符了，它会匹配尽可能少的字符
            
        *修饰符
            有时，匹配字符串中有换行符，而 . 是不包括换行符的
            只需要加个修饰符 re.S
            这个修饰符是放在 re.match 的参数里
                例如：    strs = '''hello 123 456
                                   World_This is a Regex Demo'''
                         s = re.match('he.*Demo', strs, re.S)     #re.S 大写S
                         print(s.group())
                         
            还有其他修饰符：
                re.I    使匹配大小写不敏感
                re.L    做本地化识别( locale-aware )匹配
                re.M    多行匹配，影响 ^ 和 $
                re.S    使 . 匹配包括换行符在内的所有字符
                re.U    根据 Unicode 字符集解析字符，这个标志影响 \w,\W,\b,\B
                re.X    该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解
                
        *转义匹配
            有时匹配内容中有.()\这ignore些
            这些符号需要转义，在前面加一  '\'  就好， 例如 \( \) \.
            
     
    2.search()
        match()方法是从字符串的开头开始匹配的，一旦开头不匹配，整个匹配就会失败，所以它更适合做检查某个字段是否符合某个正则表达式的规则
        而 search() 则是匹配时扫描整个字符串，然后返回第一个成功的结果
        
        使用方法自行想象
        
            例如：    s = re.search('<div id="bqid">(.*)</html>', a(这是读取一个文件的字符串), re.S)
        
        注意：AttributeError: 'NoneType' object has no attribute 'group'    这是正则表达式出现了问题
        
        
    3.findall()
        search()只是匹配第一个正则表达式的字符串，而 findall() 则返回所以符合正则表达式的字符串
        findall() 返回的列表中的每个元素都是元组类型
                  可以遍历一下列表
                  
        当给出的正则表达式中带有多个括号时，列表的元素为多个字符串组成的tuple，tuple中字符串个数与括号对数相同，字符串内容与每个括号内的正则表达式相对应，并且排放顺序是按括号出现的顺序。

        当给出的正则表达式中带有一个括号时，列表的元素为字符串，此字符串的内容与括号中的正则表达式相对应（不是整个正则表达式的匹配内容）。

        当给出的正则表达式中不带括号时，列表的元素为字符串，此字符串为整个正则表达式匹配的内容。
        
        一般没有 .group()

                  
    4.sub()              
        有时需要借助正则表达式修改文本，例如某文本的数字太多，想删除时用replace()又太烦琐
        这时可以运用 sub() 方法
            例如：    s = a1s2d34f5g6h7j
                     s = re.sub('\d+', '', s)      #sub('正则表达式,\d+表示所以数字', '代替的字符', 原来的字符串)
               
        很多时候，建议过滤文本的某些不需要的元素，再抓取有用的元素
                     
    5.compile()                 
        这个方法可以将正则字符串编译成正则表达式对象
        例如给许多字符串修改时，重复键入正则表达式太繁琐
        可以将一个字符串编译成正则表达式，然后重复利用
        
            例如：    A = 'abc 1:1'
                     B = 'abc 2:1'
                     C = 'abc 3:1'
                     pattern = re.complie('\d:\d')     #将字符串编译成正则表达式对象
                     newA = re.sub(pattern, '', A)
                     newB = re.sub(pattern, '', B)
                     newC = re.sub(pattern, '', C)
  
  
  
====    抓去首页 - 正则提取 - 写入文件 - 整合代码 - 分页爬取


                                                                                  == 解析库 ==

==================================================================================== XPath ====================================================================================================
  
====XPath，全称 XML Path Language，即XML路径语言

    XPath 的选择功能十分强大，它提供了非常简洁明了的路径选择表达式，还提供了超过100个内建函数，用于字符串，数值，时间匹配以及节点
    
====常用规则

        nodename        选取此节点的所有子节点 (nodename节点名称)
        /               从当前节点选取直接子节点
        //              从当前节点选取子孙节点
        .               选取当前节点
        ..              选取当前节点的父节点
        @               选取属性
        
        使用XPath表达式时为 .xpath('')
    
    XPath 需要引用 lxml 的 etree 模块
        from lxml import etree
        
    首先需要调用 HTML类 对一个文本进行初始化，成为XPath解析对象，而且etree的 tostring() 方法可以自动修正有缺陷的 HTML 文本，不过结果是bytes类型
        html = etree.HTML( 文本 )            #转成XPath解析对象
        result = etree.tostring(html)       #输出修正的HTML文本
        print(result.decode('utf-8'))       #tostring()的结果是bytes类型，decode()转成str类型
        
        
    还可以直接读取文件进行解析
        etree.parse() 直接接受一个文档，按照文档结构解析
        
            html = etree.parser(‘.html’, etree.HTMLParser())    #etree.HTMLPaser 大概与 etree.HTML() 一个作用
            
            
====1.所有节点
        一般用//开头的XPath规则选取所有符合要求的节点
        
            .xpath('//*')         # * 表示匹配所有节点
            .xpath('//li')        #表示选取所有li节点
            
        结果为列表形式，每个元素都是一个Element对象，可以直接用 [] 加索引取出想要的对象
        
        
    2.子节点
        通过/或//查找元素的子节点或子孙节点
        
            .xpath('//li/a')        #选择li的所有直接子节点a
            .xpath('//li//a')       #选择li的所有直接子孙节点a
            
        / 是用于选择子节点， //是用于子孙节点
    
    
    3.父节点
        知道子节点，可以用 .. 查找父节点，或者 parent::*
        
            .xpath('//a[@href='aaa']/../@class')            #获取a子节点的父节点的class属性
            .xpath('//a[@href='aaa']/parent::/@class')      #获取a子节点的父节点的class属性(一样)
    
    
    4.属性匹配
        可以用 @ 符号进行属性匹配
            
            .xpath('//li[@class='aaa']')        #@限制了li的class属性为’aaa‘，意思为匹配class为’aaa‘的li节点
            
            
    5.文本获取
        用XPath的 text() 方法获取节点中的文本
            
            错误：     .xpath('//li[@class='aaa']/text()')     
                            输出结果为 /n    
                            因为/是选择直接子节点，而内容就直接匹配到被修正的li内部的换行符，文本在a内部
                
            正确：     .xpath('//li[@class='aaa']//text()')    #把/换成//选取子孙节点
                      .xpath('//li[@class='aaa']/a/text()')   #添加/a选取a节点，再匹配a内部文本
                
    
    6.属性获取
        直接用 @ 来获取节点的属性内容
        
            .xpath('//li/a/@href')
                注意，属相匹配是中括号[]加属性名和值限定某个属性:[@href='aaa']，而这里的 @href 是获取节点的某个属性的值
    
    
    7.属性多值匹配
        有时一个属性有多个值，此时就需要 contains() 方法了
        
            .xpath('//li[contains(@class, 'aaa')]/a/text()')
            
        contains() 第一个参数传入属性名称，第二个参数传入属性值，只要此属性包含所传入属性值，就可以完成匹配
    
    
    8.多属性匹配
        有时，需要根据多个属性来确定一个节点，此时就要同时匹配多个属性，可以用XPath的运算符 and 来连接
        
            .xpath('//li[contains(@class, 'aaa') and @name='bbb']/a/text()')
    
    
    =================运算符待录
    
    
    
    9.按序选择
        有时选择某些属性时匹配到多个节点，而只想要其中的某个节点，如第二个或最后一个
        
            .xpath('//li[1]/a/text()')                  #传入数字直接选择序号，注意这里开头为1，选择第一个li节点
            .xpath('//li[last()]/a/text()')             #last()选择最后一个li节点
            .xpath('//li[position()<3]/a/text()')       #position()<3 表示选择位置小于3的li节点
            .xpath('//li[last()-2]/a/text()')           #last()-2 表示最后一个-2的位置，即倒数第三个li节点
            
            等等100多个函数
    
    
    10.节点轴选择
        XPath提供许多节点选择方法，包括获取子元素，兄弟元素，父元素，祖先元素...
        
            .xpath('//li[1]/ancestor::*')               # ancestor轴 获取所有祖先节点，其后需要两个冒号，接着是节点选择器，*表示所有，li的祖先节点包括html，body...
            .xpath('//li[1]/ancestor::div')             #这里添加限定条件 div ，结果为li的祖先div节点
            .xpath('//li[1]/attribute::*')              # attribute轴 获取所有属性值，*表示获取li节点的所有属性值
            .xpath('//li[1]/child::a[@href='aaa']')     # child轴 获取所有直接子节点，限定条件为a[@href='aaa']，意为获取属性为href='aaa'的a节点
            .xpath('//li[1]/descendant::span')          # descendant轴 获取所有子孙节点，限定条件span，结果只包含span节点，而不包含a节点
            .xpath('//li[1]/following::*[2]')           # following轴 获取当前节点之后的所有节点，这里*和索引，所以只获取了第二个后续节点
            .xpath('//li[1]/following-sibling::*')      # following-sibling轴 获取当前节点之后的所有同级节点，这里使用*，所以获取了所有后续同级节点
    
            
==================================================================================== Beautiful Soup ===========================================================================================

====强大的 Beautiful Soup 可以借助网页节结构和属性等特性来解析网页，可以不用去写一些复杂的正则表达式
    它就是 Pythoon 的一个HTML或XML的解析库
    在解析时，实际上是 BeautifulSoup() 就完成了文档的补齐更正格式
    
    Beautiful Soup 在解析时实际上依赖解析器，除了支持标准库中的HTML解析器外，还支持第三方
    
           解析器                  使用方法                                  优劣势
        python标准库           BeautifulSoup(markuo, "html.parser")      Python内置，速度适中，文档容错能力强
        lxml HTML解析库        BeautifulSoup(markuo, "lxml")             速度快，容错强，需要C语言库
        lxml XML解析库         BeautifulSoup(markuo, "xml")              速度快，唯一支持的XML解析器，需要C语言库
        html5lib              BeautifulSoup(markuo, "html5lib")         最好的容错性，以浏览器的方式解析文档，生成HTML5格式的文档，但是速度慢，不依赖外部扩展
        
         
====同样需要初始化：  BeautifulSoup( 文本 , "解析器↑")
    引用：    from bs4 import BeautifulSoup
    
        from bs4 import BeautifulSoup
        html = ".......(html文本)"
        soup = BeautifulSoup(html, "lxml")      #初始化，在下一步之前，这里更正了格式（加了body和html节点）
        print(soup.prettify)                    # prettify() 方法可以把解析的字符串以标准的缩进格式输出
        print(soup.title.string)                #这里实际上是输出HTML中的title节点的文本内容，也就是选出title，然后string提取
        
        
====节点选择器       soup = BeautifulSoup(html, "lxml")

    很清楚，直接调用节点的名称就可以选择节点元素，再调用string即可获取节点文本，如果单个节点结构层次非常清晰，可以这样
    
        print(soup.title)               #打印选择title节点的选择结果
        print(type(soup.title))         #输出它的类型
        print(soup.title.string)        #输出节点文本内容
        print(soup.head)                #选择了head节点，就是内部节点
        print(soup.p)                   #选择p节点，这里只输出了第一个匹配结果，其他的后面节点都会忽略
        
    
    1.提取信息
        *获取名称
            利用 name 属性获取节点名称
            
                print(soup.title.name)    #获取title节点的名称，(吐槽max)
                
        *获取属性
            每个节点可能有多个属性，比如id，class，可以调用attrs获取所有属性，返回结果为字典，所以能以key方式获取值  soup.p['key']
            
                print(soup.p.attrs)             #返回p的所有属性，类型是字典
                print(soup.p.attrs['key'])      #通过key获取value
                
            但这样太烦琐了，可以更简单的
            
                print(soup.p['key'])      #通过key获取value
                
            注意：有的返回结果是字符串，有的返回结果是字符串组成的列表
                 比如：name是唯一的，所以是字符串
                      而class可能有多个，所有是列表
                
        *获取内容
            string    注意：因为选择到的p节点是第一个p节点，所以文本也是第一个p节点里面的文本
        
        
    2.嵌套选择
        意思是选择某个节点内部的节点，例如输出head节点内的title文本
        
            print(soup.head.title.string)
            
        
    3.关联选择
        有时候不能做到一步就选到想要的节点，需要先选一个节点，接着以它做基点再选择子节点，父节点，兄弟节点...
        
        *子节点和子孙节点
            调用 contents 属性获取直接子节点，类型为列表
                
                print(soup.p.contents)      #返回p的直接子节点，结果是列表形式，而且包含文本 又包含节点
                                                注意：列表中都是p的直接节点a，也就是说对于a里面的span节点，并不会单独选出来，而是算在a节点的内容
                                                
            
            调用 children 属性获取直接子节点，类型为生成器类型，需要for循环遍历输出
            
                print(soup.p.children)      #返回结果为生成器类型
                for i,child in enumerate(soup.p.children):      #用enumerate()生成下标对应内容
                    print(i, child)                             #i为下标，child为内容，遍历输出
                
                输出比contents好看的列表内容，还有下标
                
            
            如果想要获取所有 子孙节点  ，调用 descendants 属性获取，类型也为生成器类型，需要for循环遍历输出
            
                print(soup.p.descendants)   						#包含子孙节点，独立出列表
                for i,child in enumerate(soup.p.descendants):      	#遍历输出，用了 enumerate() 方法
                    print(i, child)
                    
                输出包含了独立的子孙节点
                
        
        *父节点和祖先节点
			调用 parent 属性，获取父节点
				print(soup.a.parent)        #选择了a的父节点
			
			调用 parents 属性，获取祖先节点
				print(soup.a.parents)        #加了s，选择了a的祖先节点
		
        
        *兄弟节点
            兄弟节点即是同级节点
            
                list()    方法用于将元组转换为列表
            
                print(soup.a.next_sibling)                          # next_sibling 用于获取节点的 (下一个) 兄弟元素                                       
                print(soup.a.previous_sibling)                      # previous_sibling 用于获取节点的 (上一个) 兄弟元素
                print(list(enumerate(soup.a.next_siblings)))        # next_siblings 返回后面的兄弟节点          ============================================未实验
                print(list(enumerate(soup.a.previous_siblings)))    # previous_siblings 返回前面的兄弟节点
        
        
        *提取信息
            指的是获取关联元素节点的一些信息，例如文本，属性等
                print(soup.a.next_sibling)                          #输出a下一个节点的内容 ： <a....</a>
                print(soup.a.next_sibling.string)                   #输出a下一个节点的文本 
                print(soup.a.parents)                               #输出a的父节点内容或类型，书上为body节点
                print(list(soup.a.parents)[0])                      #将body节点的内容转换成列表，指定[0]，输出应为p节点
                print(list(soup.a.parents)[0].attrs['class'])       #将body节点的内容转换成列表，指定[0]的属性'class'，输出应为p节点的class属性内容
        
        
====方法选择器       soup = BeautifulSoup(html, "lxml")

    用于进行比较复杂的选择
    
    *find_all()
        顾名思义，查询所有符合条件的元素，给它传入一些属性或文本，就可以得到符合条件的元素
        它的 API 如下
        
            find_all(name, sttrs, recursive, text, **kwargs)
        
        返回结果是列表类型，且元素依然都是 bs4.element.Tag，所以可以嵌套查询内部节点或其他节点
        
        1.name
            根据节点名来查询节点元素
                print(soup.find_all(name='ul'))     #使用name参数查询节点名称
            
            嵌套查询内部节点
                for ul in soup.find_all(name='ul')  
                    print(ul.find_all(name='li'))   #嵌套查询内部节点
                    
            对于列表，遍历输出
                for ul in soup.find_all(name='ul')  
                    print(ul.find_all(name='li'))
                    for li in soup.find_all(name='li')  #遍历li
                        print(li.string)                #使用 string 输出li的内容
            
        
        2.attrs
            传入属性来查询
            这个参数的类型是字典，所传入的也要是字典
                print(soup.find_all(attrs={'id': 'list-1'}))
                
            对于di，class这些常用属性，可以不用attrs传递，可以直接传入这些参数，例如对于id
                print(soup.find_all(id='list-1'))       #这里是直接用id这个参数，这个不是API里的，意为查询id为‘list-1’的节点元素
                print(soup.find_all(class_='list-1'))   #注意，class为Python关键字，所以后面需要添加一个 下划线_ ,来使Python区分
        
        
        3.text
            用来匹配节点的文本，传入形式可以是字符串，也可以是正则表达式
                print(soup.find_all(text=re.compile('link')))       #re.compile为正则表达式
                
                返回结果依然是列表
    
    
    *find()
        这个是返回单个元素，也就是第一个匹配的元素，_all则是返回所有匹配组成的列表，也就是说find()返回的结果不再是列表
            print(soup.find(name-'ul'))     #返回第一个匹配到的结果
            
            
    另外，还有许多查询方法：
    
        find_parents()              #返回祖先节点
        find_parents()              #返回父节点
        
        find_next_siblings()        #返回 后面 所有兄弟节点
        find_next_sibling()         #返回 后面 第一个兄弟节点
        
        find_previous_siblings()    #返回 前面 所有兄弟节点
        find_previous_sibling()     #返回 前面 第一个兄弟节点
        
        find_all_next()             #返回 节点后 所有 符合条件的节点
        find_next()                 #返回 节点后 第一个 符合条件的节点
        
        find_all_previous()         #返回 节点前 所有 符合条件的节点
        find_previous()             #返回 节点前 第一个 符合条件的节点

  
====CSS选择器      soup = BeautifulSoup(html, "lxml")
    
    只需要调用 select() 方法，传入相应的CSS选择器即可
    
        print(soup.select('.a .a-b'))       #输出class为 .a 下的 .a-b CSS选择器内容
        print(soup.select('ul li'))         #输出所有ul节点下的li节点内容，结果则是所有li节点组成的列表
        print(soup,select('#list .a'))      #输出 id(#) 为list下的class为 .a 的CSS选择器的内容
    
    
    1.嵌套选择
        select() 同样支持嵌套选择，例如先选择所有ul，再遍历每个ul，选择其中的li节点
        
            for ul in soup.select('ul')
                print(ul.select('li'))
                
            不过建议还是 print(soup.select('ul li')) ，这个简洁一些
    
    
    2.获取属性
        因为节点类型是Tag类型，所以获取属性还可以用原来的方法
            for ul in soup.select('ul')
                print(ul['id'])
                print(ul.attrs['id'])       #等价上一句
    
    
    3.获取文本
        处除了可以用前面的 string 属性，此外还有一种方法， get_text()
        
            print(li.get_text())    #输出li的文本
            print(li.string)        #同价与上一句
  
  
==================================================================================== pyquery ==================================================================================================

    如果比较喜欢 CSS选择器 ，也对jQuery有所了解，那么pyquery会更适合
    
    引用：
        from pyquery import PyQuery 
        这样每次使用都得打个大写，所有给它起别名
        
        from pyquery import PyQuery as pq
        
    
====初始化
    
    pyquery的初始化方式有多种
        *字符串初始化
            字符串一般指的是纯html文本，或是其他字符串集合
        
                from pyquery import PyQuery as pq       #引入并别名
                doc = pq(html)                          #这里的html文本自行想象，与上文差不多
                print(doc('li'))                        #传入 li ，这样就选择了html文本中的所有li节点
        
        
        *URL初始化
            如果传入的是 url ，则需要指定参数 url
            
                doc = pq(url='https://www.zhihu.com')   #这里会将请求此url，得到HTML文本，以字符串形式传递给pyQuery，完成初始化
                print(doc('title'))
        
        
        *文件初始化
            还可以传递本地的文件名，需要指定参数 filename
            
                doc = pq(filename='xxx.html')           #传入内容是待解析的HTML字符串，首先获取本地文件，再以字符串形式传递给pyQuery，完成初始化
                print(doc('li'))
        
        
====基本 CSS 选择器

    print(doc('#a .a-b li'))        #意为 先选取id为 a 的节点，再选取内部class为 .a-b 的节点内的所有 li 节点
    
    它的类型是 pyQuery 
    

====查找节点

    常用的查询函数，这些函数和jQuery中的函数用法完全相同
    
    注意：这个 pyQuery 选取节点后输出几乎都把当前节点和内容一起输出
    
    *子节点
        需要用到 find() 方法，此时传入的参数是CSS选择器
        
            from pyquery import PyQuery as pq
            doc = pq(html)                      #初始化
            items = doc('.a')                   #先选取CSS选择器为 .a 的节点
            items2 = items('li')                #再选取内部的 li 节点
            
            
        其实 find() 的查找范围是节点的所有 子孙节点，如果只想查找 子节点，可以用 children() 方法
        
            items3 = items.children()
            
            items3 = items.children('.b')   #这里传入 .b ，意为在选取的所有子节点中筛选class为 .b 的节点
            
            
        总和： find() 查找所有 子孙节点 (包括子节点)
        
              children() 只查找 子节点
        
    
    *父节点
        用 parent() 获取某个节点的 父节点
        
            items = items.parent()
            
        用 parents() 获取某个节点的 祖先节点
        
            items = items.parents()
            
        传入CSS选择器即可获取符合CSS选择器的节点
    
    
    *兄弟节点
        用 siblings() 获取兄弟节点
        
            items = items.siblings()
            
        传入CSS选择器即可获取符合CSS选择器的节点       


====遍历
    
    pyquery 选择的结果有时是一个，有时是多个，且并没有返回像 Beautiful Soup 那样的列表
        
        对于单个节点来说，可以直接输出，或者以字符串输出
        
            print(items) / print(str(items))
            
        对于多个节点，则需要遍历输出，有时需要调用 items() 方法
            
            for li in doc('li').items()         #这里使用 items() 方法，应该是键值对应输出，
                print(li)                       #这里只输出一个，应该是直接打印值 而不是键 ====(未经实验)


====获取信息

    *获取属性
        提取到某个节点后，调用 attr() 方法来获取属性
        
            a = doc('.A.B a')       #这里意为选择class为 .A 和 .B 的节点内的 a节点
            print(a.attr('href'))   #传入属性的名称，得到这个属性的值，也就是把包含href的节点内容打印出来，注意是节点内容
            
            print(a.attr.href)      #此外，也可以通过调用attr属性来获取属性，这里打印href属性的值，注意是值
            
        如果，选中的是过个节点，直接调用 .attr() 方法或是直接 .attr ，输出的结果只会是第一个
        这种情况，当然是需要遍历了
        
            for item in a.attr():
                print(item.attr('href'))
    
    
    *获取文本
        获取节点后，当然是要获取内部的文本了，可以调用 text() 
        
            print(a.text())     #获取选中的a节点内部的文本信息，它会忽略a节点内部包含的html文本，只返回纯文字内容，例如a节点内有个span，这将不会输出
            
        如果某个节点内的 HTML文本 ，则用 html()
            
            print(li.html())    #输出li内部的人html文本
            
            
        注意： html() 方法返回的是第一个节点内部的HTML文本，所以需要遍历
              text() 方法返回的所有这个节点的内部纯文本，它将所有节点取得的纯文本合并返回一个字符串，中间用空格分隔，所以不怎么需要遍历
    
    
====节点操作

    pyquery提供了一系列方法来对节点进行修改，比如添加一个class，删除一个class等
    这些操作有时候会为提取信息带来极大的遍历
    
    * addClass 和 removeClass
    * attr , text , html
    * remove()
    
    
====伪类选择器



                                                                                  == 数据存储 ==
                                                                                  
==================================================================================== 文件存储 ==================================================================================================

==== TXT 文本存储

==== JSON 文件存储

==== CSV 文件存储



==================================================================================== 关系型数据库存储 ===========================================================================================

==== MySQL 的存储


==================================================================================== 非关系型数据库存储 =========================================================================================

==== MongoDB 存储

==== Redis 存储
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
