URLLIB 请求库

Urllib 是python内置的HTTP请求库，有四个模块：
            request: 模拟HTTP请求
            error: 异常处理模块
            parse: URL处理方法
            rebotparser: 识别网站的robots.txt文件，判断哪些网站可爬，哪些不能
      
      
发送请求：      
1.urloprn()
    urllib 的 request 模块可以实现发送请求并得到相应
        urllib.request.urlopen('http...') 可以抓取网页的HTML源代码
        print(type(~~~))  可以输出相应网页的类型
        print(~~~.status())  输出响应的状态码
                 .getheaders()  输出响应的头信息
                 .getheader(‘’)  输出响应头的某个的值
                 .fileno  返回文件描述
                 .fileno()  返回文件描述符

    urllib() 函数的API封装：
        urllib.request.urlopen( url,  网址URL
    
                                date=None,  以POST方式模拟表单提交方式
                                            如何需要添加该参数，需要使用 bytes() 方法将参数转化成bytes(字节流)类型
                                                bytes() 第一个参数需要是 str(字符串)类型 
                                                            如果是字典，则需要用 urllib.parse 模块里的 urlencode() 将字典参数转换成字符串： urllib.parse.urlencode({  })  //里面是字典
                                                        第二个参数是指定编码格式：encoding=''
                                                    
                                            bytes(urllib.parse.urlencode({ : }), encoding='utf8')
                                                    
                                            如果使用了，则请求方式不再是GET，而是POST
                                            http://httpbin.org/post    //可以用来测试POST请求   
                                                            /get     //也可以测试GET，依此类推
                                                                               
                                [timeout,]*,  设置超时时间，单位s，默认为全局时间    // time=
                                              超时异常为 urllib.error.URLError 属于urllib.error模块
                                              可以用try except语句跳过超时页面，使用 isinstance() 对比类型用socket模块的socket.timeout
                                          
                                cafile=None,  CA证书的文件   //请求HTTPS链接时会有用
                                capath=None,  CA证书的路径   //请求HTTPS链接时会有用
                            
                                cadefault=False,  已弃用，默认false
                                context=None  必须是ssl.SSLContext类似，用来指定ssl设置
                                )
                                
                                
2.Request()
    如果请求需要加入多个信息，如Headers，则需要 Request() 来构建一个完整的请求，方法：
        class urllib.request.Request( url,  必选参数
                                     date=None,  如果要传，必须是bytes类型，如果是字典，先用 urllib.parse.urlencode() 编码
                                     headers={},  这是一个字典，请求头，请求时可以通过headers参数直接构造，也可以通过请求实例 add_header() 方法添加
                                                  添加请求头时最常用的方法是通过修改User_Agent来伪装浏览器，默认为Python-urllib
                                     origin_req_host=None,  指请求方的 host名称 或者 ip地址
                                     unverifiable=False,  表示这个请求是否是无法验证的，默认为False，意思是用户没有足够的权限来选择接收这个请求的结果，例如请求一些图片等，就为true，因为大部分不需要验证
                                     method=None  指示请求使用的方法，是一个字符串，比如 GET, POST, PUT 等 
                                     )
                                     
                                     例如：A = urllib.request.Request(url, data=data, headers=headers, method=method)    #这里变量 A 等于一个完整的URL请求,是一个构架好的URL
                                     
                                     
3.高级用法：例如cookies处理，代理设置等
    使用 Handler 即可简单完成，理解为解决各种处理的集合，对应cookies的处理类，对应代理的处理类等
    
    在urllib.erquest模块里的 BaseHandler 是所有Handler的父类：
        它的子类有很多  例如  HTTPDefaultErrorHandler :用于处理HTTP响应错误，错误会抛向HTTPError类型错误
                           HTTPRedirectHandler  :用于处理重定向
                           HTTPCookieProcessor  :用于处理Cookies
                           ProxyHandler  :用于处理代理，默认为空
                           HTTPPasswordMgr  :用于管理密码，它维护了用户名和密码的表
                           HTTPBasicAuthHandler  :用于管理认证
                           等等 
                
    还有一个重要的类 OpenerDirector 称为 opener ，与urlopen类似，实际上urlopen就是uriilb提供的opener，urlopen处理的比较简单而已，opener实际上可以处理高级的功能 
    简单地说，需要用 Handler 创建个性化的 opener 实现高级功能，功能都集成在 Handler 。
    build_opener 可理解为实现高级操作 Handler
    
    
    *验证：需要输入账号密码才能访问的页面
            引用模块类例如    from urllib.request import HTTPBasicAuthHandler（用于管理认证）, 
                                                       HTTPPasswordMgrWithDefaultRealm（可理解为一张表）, 
                                                       build_opener（用于发送验证，仅仅是发送）
                     
            例如：p = HTTPPasswordMgrWithDefaultRealm()    #使P成为一张用于验证的表
                 p.add_password(None, url, username, password)    #给P添加参数  .add_password(变量, 变量, ...)
                 handler = HTTPBasicAuthHandler(p)    #用HTTPBasicAuthHandler()使P形成验证操作，
                 opener = build_opener(handler)    #这一步build_opener是发送请求，只是验证而已，相当于实现这个功能
                 result = opener.open(url)   #当验证成功后，用.open可以打开网页获取代码，这里相当于urlopen
                 print(result.read().decode('GBK'))    #打印代码
                                     
    *代理 
        引用模块不多说
        ProxyHandler()创建一个对象，它是一个字典
                a = ProxyHandler({                            #添加代理IP，字典类型
                        ‘http’: ‘http://192.168.1.1:1111’
                        ‘https’: ‘http://192.168.1.1:1112’
                        ......
                })
                opener = bulid_opener(a)     #这里直接openenr，之后.open() 
                
    
    *Cookies:读写cookies
            用cookie进行模拟登陆和访问，需要引用用 http.cookiesjar 中的 Cookiejar() 库
            再使用 urrlib.request 的 HTTPCookieProcessor() 处理Cookies
            写好获取命令后，就是基本的构建操作了，handler ———— opener ———— .open()
            注意：CookiesJar() 的对象是个字典，输出时常常只需要 name 和 value

                cookie = http.cookiejar.CookieJar()    #创建CookJar() 的对象，是个字典
                handler = urllib.request.HTTPCookieProcessor(cookie)    #指定操作
                opener = urllib.request.build_opener(handler)    #构建opener
                response = opener.open('http://www.baidu.com')    #打开url，实现opener操作
                for a in cookie:    #此时输出response是源代码，获取cookies的话应该是遍历输出 cookie 字典变量，可只选name 和 value
                    print(a)                         
                                     
            cookie实际上是以文本形式保存的，那么输出时可以输出成文件格式，用 .save 保存  .load 读取
            
                filename = 'cookies.txt'    #创建txt
                cookie = http.cookiejar.MozillaCookieJar(filename)    #参数为文件名时自动创建文件
                handler = urllib.request.HTTPCookieProcessor(cookie)
                opener = urllib.request.build_opener(handler) 
                response = opener.open('http://www.baidu.com') 
                cookie.save(ignore_discard=True, ignore_expires=True)    #save将cookies保存到txt
                
                ignore_discard 的意思是即使cookies将被丢弃也将它保存下来
                ignore_expires 的意思是如果cookies已经过期也将它保存并且文件已存在时将覆盖
            
            
            CookieJar有两个子类：
                        MozillaCookieJar ：保存成Mozilla型浏览器的Cookie格式    http.cookiejar.MozillaCookieJar()
                        LWPCookieJar ：保存成LWP型浏览器的Cookie格式    http.cookiejar.MozillaCookieJar.LWPCookieJar()
                                     
            当需要调用本地Cookies文件时，用 load() 读取     
                        cookie.load(‘filename’, ignore_discard=True, ignore_expires=True)
                        这一步需要放在创建 handler 之上
                                     
                                     
处理异常：
    1.URLError  
        它是 urllib 库的 eroor 模块，继承OSError，是error的基类
        他有一个属性 reason 返回错误的原因
               print(a.reason)
           
           
    2.HTTPError
          它是 URLError 的子类，专门处理 HTTP请求错误
          有三个属性： code： 返回HTTP的状态码
                    reason： 与父类一样，返回错误原因
                    headers： 返回请求头
                 
                print(a.reason, a.code, a.headers)
             
          如果想知道错误的类型，可以用上面的方法： print(type(a.reason))
       
       
解析链接
    1.urlparse()
        实现 URL 的识别和分段，返回结果为ParseResult类型的对象，是一个元组，它分为6个部分
        scheme：协议    netloc：域名      path：访问路径
        params：参数    query：查询条件   fragment：锚点
      
        https://www.baidu.com/index.html;useraa?id=10#comment
        scheme//    netloc   /   path   ;params?query#fragment
                                     
        urlparse() 的API：
        urllib.parse.urlparse( urlstring: 必选，待解析的URL
                               scheme: 默认的协议，当URl没有带协议信息时才生效，将这个参数作为默认协议，
                               allow_fragments: 意为‘不忽略’fragment，设置为false时忽略，被忽略时会被解析为 path，params 或者 query 
                            )
      
      
    2.urlunparse()  
        对立方法，即组合构造URL
        它接受的参数是一个迭代对象，长度必须为6，否则报错
      
      
    3.urlsplit()
        与 urlparse() 类似，不过它把 params 合并到 path 中，返回5个结果
        类型同样是元组
                                     
                
    4.urlunsplit()
        对立方法，5个参数拼接
                                     
  
    5.urljoin()
        提供一个base_url（基础链接）作为第一个参数，将新链接作为第二个参数
        该方法只会解析 base_url的scheme,netloc,path 并将缺失的部分补充补充内容从新链接提取
        注意：只解析base_url的前三个，就是说base_url的后面部分将被忽略，从新链接中提取
  
  
    6.urlencode()
        将一个字典序列化成GET请求
                a={'name':'ABC', 'age':'10'}
                base_url='http://www.baidu.com?'
                url=base_url + urlencode(a)    #生成GET请求
            
            
    7.parse_qs()
        对立方法，将GET请求序列化转回字典
  
  
    8.parse_qsl()
        用于将url的参数部分转化为元组组成的列表
                query = 'name=AAA&age=10'
                print(parse_qsl(query))
            
                结果[('name','AAA'),('age','10')]    #结果是个列表，列表中每个元素都是一个元组（参数名，参数值）
            
            
    9.quote()
        可以将内容转化为URL编码的格式，特别是URL带中文的
        a = '妹控'
        url = 'http://www.baidu.com/s?wd=' + quote(a)
  
  
    10.unquote()
        对应方法，用来URL解码的
        url = ‘http://www.baidu.com/s?wd=%E5%A6%B9%E6%8E%A7’
        print(unquote(url))
  
  
分析Robots协议
    1.Robots:
        Ribots协议也称作爬虫协议，机器人协议，全名：网络爬虫排除标准（Robots Exclusion Protocol）
        用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不能
        它通常是一个叫做 robots.txt 的文本文件，一般放在网站的根目录下
    
        爬虫首先检查网站根目录下是否存在 robots.txt 文件
        如果存在，则根据其定义的范围来爬取
        如果不存在，则访问可直接访问的页面
    
        robots.txt 文件主要内容：
            User-agent: *        #指定爬虫名，如果该爬虫没在这里，则不允许爬，* 指定所有
            Disallow: /         #指定不允许爬取的目录
            Allow/; /.../       #指定允许爬取的目录
        
    2.爬虫名称：
    
    
    3.robotparser:
        该模块提供一个类 RobotFileParser 
        它可以根据某网站的 robots.txt 文件来判断是否能爬
        只需要在构建方法里传入 robots.txt 的链接即可
        
            urllib.robotparser.RobotFileParser(url='')
            如果不传入默认为空，最后使用set_url()方法设置一下也可
        
        关于这个类的几个常用方法
            set_url()       :用来设置robots.txt文件的链接，如果在RobotFilePatrser()传入了，就不需要了
            read()          :读取并分析robots.txt 文件，如果不调用下面判断都会为false，所以一定要调用
            parse()         :解析robots.txt 文件，传入的参数是robots.txt 某行的内容，它会按照robots.txt。的语法规则来分析
            can_fetch()     :该方法传入入两个参数，第一个是URL-agent，第二个是要抓取的URL，返回内容是该搜索引擎是否能抓取这个URL，结果为true 和 false
            mtime()         :返回上次抓取和分析robots.txt的时间，对长时间分析和抓取的爬虫很有必要，用来定期检查和抓取最新额robots.txt
            modified()      :同样对长时间分析和抓取的爬虫很有必要，将当前时间设置为上次抓取和分析robots.txt 的时间
            
                例如：rp = RobotsFileParser('http://www.baidu.com/........')
                     rp.read()
                     print(rp.can_fetch('*', 'http://www.baidu.com/........'))
                     print(rp.can_fetch('*', 'http://www.baidu.com/......../........'))
    
    
使用requests
	简单来说，在urlib库中完成的get请求，post请求等等其他操作，都可以用requests库更简洁的完成
			例如： r = requests.get('https://www.baidu.com/')
				  print(type(r))
				  print(r.text)
				  prinf(r.cookies)
				  
				  
	.text返回的是Unicode型的数据。
	.content返回的是bytes型也就是二进制的数据
				  
	
	基本用法
		1.GET请求
			很简单 :   r = requests.get('https://www.baidu.com/')
			      	  printf(r.text)
				  
			当需要附加额外的信息时，有两种方法：
			请求时直接添加内容：r = requests.get('https://www.baidu.com/get?....')
			
			但这样不人性，一般这些信息使用字典存储的，可直接用requests的 params 参数构造：
					data = {'name': 'AAA', 'age': '10'}
					r = requests.get('https://www.baidu.com/get?....', params=data)
				
			另外，网页返回类型实际上是str类型的，但它很特殊，是json格式的，如果想直接解析返回结果得到字典格式的话，可直接调用 json() 方法
				例如: print(r.json())
				需要注意的是，如果返回的不是json，则会出现解析错误，抛出 json.decoder.JSONDEcodeError 的异常
		
		
			*抓取页面：
				请求普通的网页，则获取相应的内容
					（空）
				
			*抓取二进制数据
				图片，视频，音频等媒体文件本质是由二进制码组成，只需要读取某个媒体文件，将其二进制码保存成 .xxx 文件
					例如： r = requests.get('https://github.com/favicon.ico')
					  	  with open('favicon.ico', 'wb') as f:
					  	  f.write(r.content)
					打开favicon.ico既看到图片
				
			*添加 Headers
				有些网页因为 robots 缘故，不添加某些信息是不能访问的，例如 Useragent，方法就是添加参数，不在赘述
					header = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}
					r = requests.get('https://www.zhihu.com/')
					 

		2.POST请求
			很简单，仅仅只是换个请求而已，只不过post需要提交数据，只需要把数据集合在字典里做参数就好，方法只是换成post
				data = {'name': 'AAA', 'age': '10'}
				r = requests.post('https://httpbin.org/post', data=data)
			
			
		3.响应
			例如：   r = requests.get('https://www.zhihu.com')
					print(type(r.status_code), r.status_code)
					print(type(r.url), r.url)
					
			其中，requests还提供了内置的状态码查询对象 requests.codes 
				例如：  r = requests.get('https://www.zhihu.com/')
				   	   exit() if not r.status_code == requests.codes.ok else print('OK')
				   
				       if not 如果不，状态码如果不相等200，则exit()，否则输出OK
				       相应的查询条件在128页
                       
                       5
    高级用法
        与urllib差不多，关键是简洁
        
        1.文件上传
            例如：    files = {'file': open('MEIKONG.jpeg', 'rb')}
                     r = requests.post('https://www.zhihu.com/', files=files)
                     
        2.Cookies
            获取cookies不再赘述
            
            可以直接用cookies来维持登录状态，这首先需要登录某个网站后，把cookie复制下来
            利用 headers参数 即可正常登录
            
            还可以通过cookies参数来设置，这需要构造RequestsCookieJar对象，这相对烦琐，书中132页
            
        3.会话维持
            每一次get() 或post() 请求都相当于用一个新的浏览器打开新的页面，这使得不能在同一会话中进行访问，也就是不能用同一个浏览器，打开新页面
            Session 可维持同一个会话
                例如：    requests.get('https://httpbin.org/cookies')             #请求一个网页的cookies
                         r = requests.get('https://httpbin.org/cookies/123')     #请求另一个网页的cookies
                         输出cookies，并不能获取当前的cookies
                         
                         r = requests.Session()                       #设置Session对象，注意大写
                         r.get('https://httpbin.org/cookies/123')     #请求第一个cookies
                         s = r.get('https://httpbin.org/cookies')     #第二个
                         输出cookies，成功返回当前cookies
        
        4.SSl证书验证
            requests 的 verify 参数用来控制是否检查证书，结果为 true 和 false
            有些没被官方CA机构信任的网站，访问时会出现 SSlError 证书验证错误，只需要将 verify 设置成false 即可跳过验证
                例如：
            
            不过，这样会发出一个建议指定证的警告，可以设置忽略警告来屏蔽，需要requests.packages 的 urllib3
                例如：    from requests.packages import urllib3
                         
                         urllib3.disable_warings()
                         response = requests.get('http://www.12306.cn', verify=False)
                         print(response.ststus_code)
                         
            还有其他屏蔽警告方式，书中135页
        
        5.代理设置
            相应的是 proxies参数 ，此对象是个字典，存放代理ip，格式：{'http': 'http://10.10.1.10:1111'}
            
            若代理需要 HTTP Basic Auth (HTTP基本认证) ，可使用类似http://user:password@host:port 这样的语法来设置代理
                例如：    proxies = {'http': 'http://user:password@host:port'}
            
            requests 还支持 SOCKS 协议代理，需要先安装socks库     pip3 install 'requests[socks]'
                例如：    proxies = {'http': 'socks5://user:password@host:port'}
                                             ↑这里换成了socks协议
            
            最后   requests.get('http://www.taobao.com', proxies=proxies)  发送请求
            
        6.超时设置
            相应参数为 timeout   (与urllib.request一样)
            
            详细书中136页
            
        7.身份验证
            对应参数 auth
                例如：  r = requests.get('http://localhost:5000', auth=('username', 'password'))
                
            requests 还提供了其他认证方式，例如 OAuth，需要安装对应的包  pip3 install requests_oauthlib
            元组组成账号密码，最后 get 发送，不赘述
            
        8.Prepared Request
            将请求表示为数据结构，在 requests 中叫做 Prepared Request
                例如：   url = 
                        data = 
                        headers = 
                        s = Session()                                                         #会话维持
                        request = Request('请求方式', 'url', 'data=data', 'headers=headers')   #Request 组合结构
                        prepped = s.prepare_request(request)                                #调用Session的prepare_request()方法将其转换成Prepared Request对象
                        r = s.send(prepped)                                                  #调用Session的 .send()方法发送
                        print(r.text)


正则表达式
    这个是处理字符串的强大工具，大概是，以某种表达方式去匹配字符串内容，返回符合正则表达式的字符串
    需要用到 re 库
 
  
  
  
  
  
  
  
  
  
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
                                     
